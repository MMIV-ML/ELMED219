{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2024-01-15, Arvid Lundervold\n",
    "\n",
    "\n",
    "[![Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMIV-ML/ELMED219/blob/main/Lab3-GenAI/lab3-genAI-nlp-explore.ipynb)\n",
    "\n",
    "\n",
    "# Lab 3 Generative AI - NLP explore (GPT-2)\n",
    "\n",
    "We here assume PyTorch and the [transformers](https://huggingface.co/docs/transformers/index) library (from Hugging Face, which provides easy access to pre-trained models and their tokenizers) are installed, if not, run the following cell:\n",
    "\n",
    "```python\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the Token ID of the word \"patient\" in a GPT model using Python, you would typically use a tokenizer compatible with the GPT model. One of the most common libraries for this purpose is Hugging Face's Transformers library, which provides easy access to pre-trained models and their tokenizers.\n",
    "\n",
    "Here's a Python code snippet that demonstrates how to find the Token ID for \"patient\" using the GPT-2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID for 'The': [464]\n",
      "Token ID for 'patient': [26029]\n",
      "Token ID for 'has': [10134]\n",
      "Token ID for 'a': [64]\n",
      "Token ID for 'fever': [69, 964]\n",
      "Token ID for '.': [13]\n"
     ]
    }
   ],
   "source": [
    "def get_token_id(word):\n",
    "    # Load the tokenizer for GPT-2\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode the word to get its Token ID\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "    return token_id\n",
    "\n",
    "# Example usage\n",
    "\n",
    "for word in [\"The\", \"patient\", \"has\", \"a\", \"fever\", \".\"]:\n",
    "    token_id = get_token_id(word)\n",
    "    print(f\"Token ID for '{word}': {token_id}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To retrieve the embedding vector for a specific token (like \"patient\" with Token ID 26029) in a model like GPT-2, you need to access the model's embedding layer. This layer maps each Token ID to a high-dimensional vector, which represents the token in the model's learned feature space.\n",
    "\n",
    "Here's how you can retrieve the embedding vector for the word \"patient\" in GPT-2 using Python and the Hugging Face Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector for 'patient': shape = torch.Size([1, 768]) tensor([[-1.2062e-01, -3.6002e-01,  1.5080e-01, -9.3105e-02, -6.0587e-02,\n",
      "         -2.5183e-01, -3.5659e-01, -1.8158e-01, -2.6167e-01,  8.3643e-02,\n",
      "          3.1463e-01,  2.2771e-01,  4.7810e-03, -1.3722e-01, -2.8546e-02,\n",
      "         -8.3280e-02, -4.7087e-02, -1.6795e-01,  7.5017e-03,  8.9747e-02,\n",
      "         -1.5150e-01,  1.1515e-01,  1.2865e-01, -1.0377e-01, -1.1852e-01,\n",
      "         -1.0153e-01,  1.2508e-01, -1.3948e-01, -4.8302e-02,  9.5049e-02,\n",
      "         -2.9984e-02, -4.5584e-02,  3.0392e-02, -1.2178e-03, -6.5477e-02,\n",
      "          7.5003e-02, -3.1934e-01, -1.2183e-02,  2.3057e-01,  7.4451e-02,\n",
      "         -3.0815e-01, -6.5707e-02,  1.4448e-01, -1.9877e-02, -7.7836e-02,\n",
      "          1.1676e-01,  1.0865e-02, -1.6200e-01, -2.0710e-01, -2.1135e-01,\n",
      "         -1.4299e-01,  9.1997e-02,  1.2531e-01, -4.3654e-02,  4.0772e-02,\n",
      "         -3.0279e-01,  7.5637e-02, -1.2184e-01,  1.0579e-01,  7.6688e-02,\n",
      "         -1.2078e-01, -4.7883e-02, -3.6084e-03,  6.2679e-02,  3.9213e-01,\n",
      "         -9.3508e-02, -5.0395e-03, -5.3647e-02,  9.4140e-02,  1.9427e-02,\n",
      "         -1.6041e-01, -9.9473e-03,  7.5020e-02, -1.6925e-02, -3.9782e-02,\n",
      "         -1.0571e-01, -1.6205e-01,  2.6679e-01,  1.3723e-01, -1.2981e-01,\n",
      "          6.5941e-02,  1.3092e-01, -1.6989e-01,  1.9796e-01,  9.5423e-02,\n",
      "         -4.3245e-02, -1.9928e-01,  3.8023e-01, -1.2966e-02, -8.2294e-02,\n",
      "          2.0421e-04,  7.8159e-02,  1.3209e-01,  2.6804e-01,  1.3159e-02,\n",
      "         -7.4559e-02, -1.4858e-01,  1.2447e-01, -1.3361e-01, -1.4974e-02,\n",
      "          9.0648e-02,  2.4060e-02, -7.4625e-02, -6.1815e-02, -2.5378e-03,\n",
      "          1.3761e-01, -2.4312e-02, -1.3617e-01,  1.4472e-01,  2.5375e-01,\n",
      "          5.6096e-02,  8.6972e-02, -3.2148e-01, -1.3955e-01,  1.2278e-02,\n",
      "         -2.8702e-01, -1.2017e-01, -2.0184e-02,  4.5083e-02,  5.4774e-02,\n",
      "          5.9150e-02,  4.7122e-03, -1.9644e-01,  1.6247e-01, -1.2189e-01,\n",
      "         -2.7498e-01, -7.5955e-02, -1.1329e-01, -7.7399e-02, -1.1339e-01,\n",
      "         -4.0497e-02, -1.2341e-01, -5.7403e-02,  5.8781e-02,  8.0579e-02,\n",
      "          2.3641e-01,  9.3032e-03, -1.2810e-01, -2.6579e-01, -1.5323e-01,\n",
      "          8.9362e-02, -3.8313e-02,  8.8713e-02,  3.1333e-01,  1.1264e-02,\n",
      "         -2.3205e-01, -6.7145e-02, -6.3484e-02, -5.6505e-02,  3.0942e-01,\n",
      "          2.1646e-01, -2.0391e-01,  1.6480e-01, -1.5580e-01,  1.2218e-01,\n",
      "          7.3810e-02, -7.0206e-02,  2.7871e-01,  2.0432e-01, -2.7746e-01,\n",
      "         -5.4197e-02,  3.3121e-02,  4.2000e-02, -1.8387e-01,  8.0776e-03,\n",
      "         -5.3866e-02,  1.7300e-01,  2.9250e-01, -3.8724e-02,  2.9339e-01,\n",
      "          3.7048e-02, -1.1128e-01, -3.7400e-02, -9.7078e-02, -2.3253e-01,\n",
      "          5.2306e-02,  1.3541e-01,  7.3520e-02,  1.4827e-02, -6.6998e-02,\n",
      "          1.6122e-01, -5.0251e-03, -2.9353e-02, -3.1384e-02,  1.9918e-01,\n",
      "         -6.8549e-02,  1.3494e-02,  4.5972e-02, -1.8126e-01, -2.5608e-01,\n",
      "          2.4086e-01,  1.5001e-01,  9.6827e-02, -4.2034e-02,  2.8934e-01,\n",
      "          8.7303e-02,  8.5283e-02,  7.3247e-02,  4.6714e-02,  1.9381e-01,\n",
      "         -5.2641e-02, -5.6377e-02,  1.8121e-01, -2.1807e-01, -2.3473e-01,\n",
      "         -2.0521e-02,  3.2807e-02, -4.8788e-02, -6.7064e-02,  2.2021e-02,\n",
      "          1.3453e-01,  2.0470e-01, -1.7932e-01,  1.6940e-02,  3.2082e-01,\n",
      "         -2.5640e-01, -6.2908e-02, -2.9440e-02,  1.8989e-01, -2.7376e-03,\n",
      "         -2.3992e-01,  1.5966e-01,  2.0829e-01, -4.0667e-01, -2.6062e-01,\n",
      "         -1.3317e-01, -5.1763e-02,  1.5360e-01,  1.6231e-01, -1.4335e-01,\n",
      "         -2.0161e-01,  1.8757e-02, -1.1953e-01,  2.7035e-01, -7.8183e-02,\n",
      "          4.5541e-02, -1.3730e-01,  6.8330e-02, -1.2490e-01, -1.4723e-01,\n",
      "         -5.7847e-02, -2.1497e-01,  1.2992e-01, -1.2113e-01, -6.3360e-02,\n",
      "         -3.7383e-02, -4.6964e-02,  4.1853e-03,  7.6563e-02,  4.2887e-02,\n",
      "          1.7794e-01, -1.5552e-02,  7.4254e-02, -4.0226e-02,  4.4128e-02,\n",
      "         -2.2015e-01,  1.1349e-01, -2.2488e-01, -8.7438e-02,  2.3417e-01,\n",
      "          3.6819e-01,  2.7149e-01, -2.0762e-01, -1.7207e-02,  6.5015e-02,\n",
      "         -1.8271e-01,  3.7847e-01, -2.3949e-01,  1.3649e-01,  1.2953e-01,\n",
      "          1.1679e-01, -1.2607e-01, -5.1937e-02, -7.9846e-02, -1.3700e-01,\n",
      "          8.5248e-03, -4.8298e-02, -3.0482e-03, -1.8042e-01,  5.2933e-02,\n",
      "          6.5120e-02,  5.3945e-02, -2.9984e-01,  7.3848e-02, -2.0269e-01,\n",
      "         -8.6791e-03, -1.4568e-01, -1.9561e-01, -5.3408e-01,  2.0837e-01,\n",
      "         -1.2524e-01, -4.7929e-02, -1.7974e-02,  2.4143e-01, -2.2251e-01,\n",
      "          8.0843e-02,  1.5122e-01, -1.6631e-02, -8.0199e-02, -6.4640e-02,\n",
      "          1.4454e-01, -2.0936e-03,  7.6550e-02, -1.0228e-01,  3.8653e-03,\n",
      "         -1.3962e-01, -2.6068e-01, -1.5732e-01,  1.4959e-01, -9.2489e-02,\n",
      "          5.5791e-02, -4.5349e-02, -3.2817e-02, -4.5177e-02,  2.0615e-01,\n",
      "          4.3460e-02,  1.6539e-01,  9.2441e-02, -1.6014e-01,  2.8917e-01,\n",
      "          2.4953e-01,  1.4856e-01,  6.9567e-02,  1.5407e-01,  1.6883e-01,\n",
      "          1.6623e-01, -7.8801e-01,  8.5617e-02,  2.3935e-01,  3.4170e-02,\n",
      "         -3.0340e-01,  2.9280e-02, -4.4364e-02, -4.9960e-02,  7.3601e-04,\n",
      "          1.4631e-01,  7.6508e-02,  1.3444e-01,  2.5493e-01, -1.6384e-02,\n",
      "         -1.4872e-01,  5.7938e-03, -1.1431e-01, -9.3460e-02, -1.2727e-01,\n",
      "         -1.8469e-01,  1.4887e-01,  1.3071e-01, -6.0675e-02, -1.3136e-01,\n",
      "         -8.8339e-02, -2.7648e-01,  2.4137e-01,  5.6358e-02, -5.4359e-02,\n",
      "         -2.3666e-01,  8.5758e-02,  1.7009e-01, -1.0928e-01, -1.6554e-01,\n",
      "          1.8262e-01,  5.7628e-02, -2.2975e-01,  7.4329e-02,  1.3190e-02,\n",
      "          1.7732e-01,  2.5688e-02, -2.1861e-01,  1.9297e-02,  2.0160e-02,\n",
      "          1.9674e-01, -1.1141e-02, -1.7788e-02,  3.3891e-01,  1.8477e-01,\n",
      "         -1.8134e-02, -3.2731e-02, -3.0425e-02, -1.3617e-01,  4.4614e-02,\n",
      "          2.4490e-01, -3.3592e-01,  1.2924e-01,  3.8149e-02, -8.7198e-03,\n",
      "          1.3741e-01,  5.8920e-03, -9.9148e-02, -2.2990e-02,  1.7164e-01,\n",
      "          1.7126e-01,  1.3957e-01,  2.1994e-01, -9.3015e-02,  2.4617e-02,\n",
      "          6.8969e-02, -4.5611e-02, -2.8059e-01, -1.0583e-01, -2.9471e-02,\n",
      "          4.4365e-02,  4.9640e-02, -3.8099e-02,  1.2843e-01,  3.0812e-02,\n",
      "         -3.9482e-02, -7.0820e-02,  1.6004e-01, -2.7134e-01, -1.3037e-01,\n",
      "          6.1128e-02, -3.5319e-02,  1.9556e-01,  8.7218e-02,  1.2092e-01,\n",
      "         -6.3431e-02, -7.8918e-02, -1.1028e-01, -3.9321e-02, -1.4106e-02,\n",
      "         -4.6780e-02, -1.2103e-01,  2.8521e-01,  2.8068e-01, -1.3517e-01,\n",
      "         -7.7614e-02, -1.3138e-01, -1.9575e-01,  1.8698e-01, -1.1783e-03,\n",
      "         -2.5710e-01,  1.4072e-01, -2.8202e-01,  2.9492e-02,  9.0350e-02,\n",
      "         -7.1170e-02, -6.6485e-02, -7.3385e-02,  2.4661e-01, -9.6402e-02,\n",
      "         -1.6496e-01, -2.3498e-01, -1.5066e-01, -9.7797e-02,  3.8597e-03,\n",
      "          1.2735e-02, -1.2647e-01, -3.2043e-01,  3.3300e-01,  2.1276e-02,\n",
      "          2.3750e-01,  1.4340e-01,  9.5423e-02,  1.0741e-02,  2.2423e-02,\n",
      "         -1.6439e-01,  1.4964e-02,  3.2158e-01,  3.6166e-02, -2.3376e-01,\n",
      "         -1.5271e-01,  9.9250e-03, -8.4408e-02, -2.9665e-03, -2.2960e-02,\n",
      "          1.4773e-01, -1.5879e-01,  6.9840e-02,  3.6044e-02,  7.3679e-02,\n",
      "         -9.7756e-02, -3.4870e-01, -1.1195e-01,  7.3584e-02,  2.5155e-03,\n",
      "         -1.8388e-01,  7.5806e-03,  1.0390e-02,  1.9639e-01,  1.5526e-01,\n",
      "          3.2943e-01, -3.5869e-01, -5.7234e-02,  1.5930e-02, -1.6506e-01,\n",
      "          3.4750e-02,  2.6485e-01,  8.0315e-02, -3.9265e-02,  1.7298e-01,\n",
      "          8.3585e-03, -4.4113e-02,  1.7365e-01, -9.5211e-02, -1.3420e-01,\n",
      "          6.4914e-02, -2.0654e-01,  2.2468e-01,  6.9091e-02, -8.8179e-02,\n",
      "         -7.2571e-02,  8.7324e-02, -1.7455e-01,  1.3466e-01, -4.1483e-02,\n",
      "         -2.0481e-01, -4.3202e-01, -3.2010e-01, -3.1326e-01,  2.3564e-02,\n",
      "         -4.3776e-02,  2.1143e-01, -2.2315e-01, -9.7908e-02, -1.2899e-01,\n",
      "          2.2146e-01, -3.4824e-02,  2.9442e-01,  2.1699e-01, -1.3795e-02,\n",
      "         -1.2353e-01,  1.7763e-01, -1.3145e-01, -1.1513e-01,  2.4366e-02,\n",
      "         -2.3887e-02,  2.5255e-01,  1.7574e-01, -5.5833e-02,  4.7130e-02,\n",
      "         -1.9272e-01,  5.4779e-02, -1.5612e-01,  7.8634e-02, -5.8503e-02,\n",
      "          5.4371e-02,  1.6957e-01, -1.5636e-01, -1.3442e-01, -7.7718e-02,\n",
      "         -3.8264e-02, -1.6725e-01, -6.8400e-02,  1.0073e-01,  9.3402e-04,\n",
      "          2.7076e-03, -2.9568e-02, -2.2615e-01,  2.1378e-01,  9.6874e-02,\n",
      "         -1.0420e-01, -2.9930e-01,  7.6399e-02,  1.4839e-01,  2.6741e-01,\n",
      "         -4.9437e-02,  9.2604e-02,  2.1248e-01, -1.2058e-02, -3.3273e-02,\n",
      "          1.0381e-01, -2.0746e-01,  5.2306e-02,  1.6304e-02,  6.2790e-02,\n",
      "          6.3078e-02, -2.7914e-01,  2.8354e-01,  2.6535e-01,  5.3627e-02,\n",
      "          4.2675e-02,  1.6086e-01, -1.3038e-01,  1.2269e-01, -3.1686e-02,\n",
      "          1.8860e-01,  3.2005e-02, -4.8063e-02,  4.9249e-02,  2.2017e-02,\n",
      "          2.8183e-01,  4.4848e-02, -9.1920e-02, -2.6536e-02,  1.3835e-01,\n",
      "          6.3269e-02,  6.6897e-02, -2.1413e-01, -2.1091e-01,  2.8395e-02,\n",
      "          2.0950e-02,  5.8547e-02,  1.7249e-01,  4.4747e-01,  2.4156e-01,\n",
      "          1.1126e-01,  4.5588e-02,  1.9372e-01, -1.9592e-01, -1.2583e-01,\n",
      "          3.3907e-01, -2.2912e-01,  2.5494e-01, -2.8483e-01,  1.0027e-01,\n",
      "         -1.5194e-02, -3.0577e-03, -2.8300e-02, -1.8435e-01, -1.3255e-01,\n",
      "          1.0419e-01,  1.3229e-01,  3.0359e-01, -1.8900e-01, -1.5670e-01,\n",
      "          1.4025e-01,  2.3933e-01, -5.8758e-02,  1.6697e-01,  1.1285e-01,\n",
      "         -1.3004e-02, -1.9086e-01, -1.7226e-03, -4.4202e-02,  1.4417e-01,\n",
      "          1.3250e-01, -1.4028e-01,  2.6028e-01,  7.9476e-02,  5.0353e-02,\n",
      "         -9.0812e-02, -8.9195e-02, -2.1714e-01,  2.2166e-01, -4.2666e-02,\n",
      "          1.6477e-01,  1.8018e-01,  1.9615e-02, -3.8984e-02,  2.6093e-02,\n",
      "          5.7593e-02,  1.4394e-02,  5.4691e-02,  2.1988e-01,  1.1085e-01,\n",
      "          1.0979e-01,  7.6745e-02,  1.3953e-01,  1.3076e-01,  3.5038e-01,\n",
      "         -3.5463e-03,  2.2679e-01, -2.0657e-01, -1.3848e-01,  7.6906e-02,\n",
      "          8.6594e-04, -5.8987e-02, -5.4194e-03,  4.7278e-02, -2.8361e-01,\n",
      "          1.8870e-01,  1.4168e-01,  6.0899e-02, -6.6572e-02, -1.8278e-01,\n",
      "          9.4231e-02, -1.5877e-01,  3.2327e-02, -2.5044e-01, -2.4497e-01,\n",
      "          8.9294e-02, -1.4901e-02, -1.0431e-01, -2.7383e-01,  2.5630e-01,\n",
      "          9.7500e-02,  6.8603e-02, -3.1331e-03,  8.6250e-03, -2.3569e-01,\n",
      "          1.5616e-01,  9.3152e-02,  1.2113e-01,  8.5962e-02, -3.1079e-01,\n",
      "         -6.1589e-02, -5.5828e-02,  4.1948e-02,  6.9149e-02, -4.3765e-02,\n",
      "         -3.1679e-01,  4.1084e-02, -3.2848e-01,  7.8452e-02,  3.5069e-02,\n",
      "         -2.6601e-01, -7.3685e-02, -1.7283e-01, -6.6236e-02, -6.7039e-02,\n",
      "         -2.5493e-01, -2.8620e-01, -7.6381e-02,  1.4495e-01, -3.2354e-01,\n",
      "          1.0809e-01,  2.7301e-01, -9.4669e-02, -5.4031e-02,  3.1570e-01,\n",
      "          3.0517e-02,  1.3442e-02, -4.4420e-02,  1.4308e-01,  9.0900e-02,\n",
      "          1.2455e-01, -9.8657e-02,  4.8082e-02, -1.6469e-01,  6.0381e-02,\n",
      "          9.2114e-02,  2.1968e-01,  9.2287e-02,  9.1840e-02, -8.8844e-02,\n",
      "         -2.7103e-01, -2.1487e-01,  5.4641e-02,  1.9650e-01,  1.3236e-01,\n",
      "          1.2299e-02, -1.1874e-02, -3.4973e-01, -2.1113e-01, -3.3487e-02,\n",
      "         -2.9633e-02, -8.3138e-02, -9.7628e-03,  4.3653e-02,  1.3441e-01,\n",
      "          3.3777e-01,  7.6425e-02,  1.4994e-01, -1.1784e-01,  2.4066e-02,\n",
      "          1.7534e-01,  1.5217e-01, -1.0778e-01,  2.5008e-01,  1.6412e-01,\n",
      "          3.7569e-02,  2.3517e-01,  1.9604e-01,  1.0213e-01, -2.2925e-01,\n",
      "          2.9165e-02, -2.3061e-01, -1.0446e-01,  4.9158e-02,  1.9613e-02,\n",
      "          7.4300e-02, -2.7997e-01, -2.5813e-01, -5.0060e-02,  6.4030e-02,\n",
      "          8.0653e-02, -2.8974e-02,  6.3425e-02]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "def get_embedding_vector(word):\n",
    "    # Load the tokenizer and model for GPT-2\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Encode the word to get its Token ID\n",
    "    token_id = tokenizer.encode(word, add_special_tokens=False)\n",
    "\n",
    "    # Retrieve the embedding for the Token ID\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    word_embedding = embeddings(torch.LongTensor(token_id))\n",
    "\n",
    "    return word_embedding\n",
    "\n",
    "# Example usage\n",
    "word = \"patient\"\n",
    "embedding_vector = get_embedding_vector(word)\n",
    "print(f\"Embedding vector for '{word}': shape = {embedding_vector.shape} {embedding_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the embedding vector is a high-dimensional tensor (usually several hundred dimensions), and its values are learned during the model's training process. The specific values in this vector capture semantic and syntactic information about the word as learned by the model from its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elmed219",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
